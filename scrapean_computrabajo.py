# -*- coding: utf-8 -*-
"""Scrapean_Computrabajo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tYd4tjNg-j4uiQXJUDnfhRMjxjFO55gL
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time


#Implementación de Categorias para el filtro.

categoria =[
    "administracion-y-oficina",
    "almacen-logistica",
    "atencion-a-clientes",
    "callcenter-telemercadeo",
    "compras-comercio-exterior",
    "construccion-y-obra",
    "economia-y-contabilidad",
    "direccion-y-gerencia",
    "arte-y-diseno-y-medios",
    "educacion-y-universidad",
    "hosteleria-y-turismo",
    "informatica-y-telecom",
    "ingenieria-y-tecnico",
    "cientifico-y-investigacion",
    "legal-y-asesoria",
    "mantenimiento-y-reparaciones-tecnicas",
    "medicina-y-salud",
    "mercadeo-publicidad-comunicacion",
    "produccion-operaciones",
    "recursos-humanos",
    "servicios-generales-aseo-y-seguridad",
    "marketing-y-ventas"
]


# Función para scrapear una categoría completa
def scrap_categoria(categoria, max_pages=50):

  url =f'https://co.computrabajo.com/salarios-de-{categoria}?by=averageDesc&p={{}}'
  headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
            }
  profesiones = []

  # Recorremos páginas (ajusta el rango si quieres más)
  for page in range(1, max_pages+50):
      res = requests.get(url.format(page), headers=headers)
      soup = BeautifulSoup(res.text, "html.parser")

      #Revisión Fila x Fila teniendo en cuenta la clave de estructura de "div.dTable.w_100.pb40.row"
      filas = soup.select("div.dTable.w_100.pb40.row")

      for fila in filas:
          # Buscamos todos los bloques de filas
          puestos = fila.select_one("div.tablec.w_40.vmid.fw_b.fs16 a") or fila.select_one("div.tablec.w_40.vmid.fw_b.fs16 span")
          salarios = fila.select_one("div.tablec.w_30.vmid p.fw_b.fs18")
          n_salarios = fila.select_one("div.tablec.w_30.vmid span.fc80")
          rangos_salariales = fila.select_one("div.dFlex.tj_fx.fs12.fc_aux.mt5")

          #Condicional para verificación o salteo de dato en caso de no tener una estructura adecuada en base a la variable "Puestos"
          if not puestos or not salarios:
            continue # Las variables de puestos y salarios se encuentran vacios

          salario_min = None
          salario_max = None

## Ejempo de (get_text)
#from bs4 import BeautifulSoup
#html = "<p>Hola <b>Mundo</b></p>"
#soup = BeautifulSoup(html, "html.parser")
#parrafo = soup.find("p")
#print(parrafo.get_text())
# SALIDA DEL PRINT===================================== "Hola Mundo"


          if rangos_salariales:
            valores = [x.get_text(strip=True)for x in rangos_salariales.select("p")]
            if len(valores)>0:
              salario_min = valores[0]
            if len(valores)>1:
              salario_max = valores [1]


          profesiones.append({
              "CATEGORIA": categoria,
              "puesto": puestos.get_text(strip=True),
              "salario": salarios.get_text(strip=True),
              "n_salarios": n_salarios.get_text(strip=True),
              "salario_min": salario_min,
              "salario_max": salario_max
            })

      time.sleep(1) #Delay de 1 segúndo por la carga de cada pagina.

  return profesiones #devolucion de la lista por categoria

#Almacenamiento de todos los datos
# ---- Scrapeamos TODAS las categorías ----
todos_datos=[]
for cat in categoria:
  print(f'Scrapeando categoria: {cat}')
  datos_cat= scrap_categoria(cat, max_pages=2)
  todos_datos.extend(datos_cat)

df = pd.DataFrame(todos_datos)
print("Total profesiones encontradas:", len(df))
print(df.head(162))

# Guardar en csv
df.to_csv("salarios_colombia.csv", index=False)
print("Archivo generado y guardado.")

x = True

if not x:   # equivale a if x == None o if x == False
    print("x está vacío o es None")

else:
  print("La variable contiene un dato")